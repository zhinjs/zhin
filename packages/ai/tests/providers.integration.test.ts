/**
 * AI Providers é›†æˆæµ‹è¯•ï¼ˆçœŸå® API è°ƒç”¨ï¼‰
 * 
 * ä½¿ç”¨ test-bot çš„ç¯å¢ƒé…ç½®è¿›è¡ŒçœŸå® API æµ‹è¯•
 * 
 * è¿è¡Œæ–¹å¼ï¼ˆéœ€è¦ç½‘ç»œæƒé™ï¼‰ï¼š
 * pnpm test packages/ai/tests/providers.integration.test.ts
 * 
 * æ³¨æ„ï¼šæ­¤æµ‹è¯•éœ€è¦ç½‘ç»œè®¿é—®ï¼Œåœ¨ sandbox ä¸­ä¼šè‡ªåŠ¨è·³è¿‡
 */
import { describe, it, expect, beforeAll } from 'vitest';
import { OllamaProvider } from '../src/providers/ollama.js';
import type { ChatMessage } from '@zhin.js/core';

// Ollama é…ç½®
const OLLAMA_CONFIG = {
  baseUrl: 'https://ollama.l2cl.link',
  models: ['qwen2.5:7b', 'qwen3:8b'],
};

// å…¨å±€çŠ¶æ€
let canRun = false;
let provider: OllamaProvider | null = null;

// è·³è¿‡æ£€æŸ¥
const skipIfNoNetwork = (ctx: any) => {
  if (!canRun) {
    ctx.skip();
    return true;
  }
  return false;
};

beforeAll(async () => {
  try {
    const controller = new AbortController();
    const timeout = setTimeout(() => controller.abort(), 5000);
    const response = await globalThis.fetch(`${OLLAMA_CONFIG.baseUrl}/api/tags`, {
      signal: controller.signal,
    });
    clearTimeout(timeout);
    canRun = response.ok;
    
    if (canRun) {
      provider = new OllamaProvider({
        baseUrl: OLLAMA_CONFIG.baseUrl,
        models: OLLAMA_CONFIG.models,
      });
      console.log('âœ… Ollama æœåŠ¡å¯ç”¨');
    }
  } catch {
    canRun = false;
    console.log('âš ï¸ ç½‘ç»œä¸å¯ç”¨ï¼Œè·³è¿‡é›†æˆæµ‹è¯•');
  }
});

describe('Ollama Provider é›†æˆæµ‹è¯•', () => {
  describe('åŸºæœ¬èŠå¤©', () => {
    it('åº”è¯¥èƒ½è¿›è¡Œç®€å•å¯¹è¯', async (ctx) => {
      if (skipIfNoNetwork(ctx)) return;
      
      const response = await provider!.chat({
        model: OLLAMA_CONFIG.models[0],
        messages: [{ role: 'user', content: 'ä½ å¥½ï¼Œä¸€å¥è¯ä»‹ç»è‡ªå·±' }],
        max_tokens: 100,
      });

      expect(response.choices[0].message.content).toBeTruthy();
      console.log('ğŸ“ AI å›å¤:', response.choices[0].message.content);
    }, 30000);

    it('åº”è¯¥èƒ½å¤„ç†å¤šè½®å¯¹è¯', async (ctx) => {
      if (skipIfNoNetwork(ctx)) return;
      
      const messages: ChatMessage[] = [
        { role: 'user', content: 'è®°ä½ï¼šæˆ‘å«å°æ˜ã€‚ç®€çŸ­å›å¤ã€‚' },
      ];

      const r1 = await provider!.chat({
        model: OLLAMA_CONFIG.models[0],
        messages,
        max_tokens: 30,
      });

      messages.push({ role: 'assistant', content: r1.choices[0].message.content as string });
      messages.push({ role: 'user', content: 'æˆ‘å«ä»€ä¹ˆï¼Ÿ' });

      const r2 = await provider!.chat({
        model: OLLAMA_CONFIG.models[0],
        messages,
        max_tokens: 30,
      });

      const reply = r2.choices[0].message.content as string;
      console.log('ğŸ“ å¤šè½®å¯¹è¯:', reply);
      expect(reply.toLowerCase()).toContain('å°æ˜');
    }, 120000);
  });

  describe('å·¥å…·è°ƒç”¨', () => {
    it('åº”è¯¥èƒ½è°ƒç”¨è®¡ç®—å™¨å·¥å…·', async (ctx) => {
      if (skipIfNoNetwork(ctx)) return;
      
      const response = await provider!.chat({
        model: OLLAMA_CONFIG.models[0],
        messages: [
          { role: 'system', content: 'ä½¿ç”¨ calculator å·¥å…·è®¡ç®—ã€‚' },
          { role: 'user', content: 'è®¡ç®— 15 * 8' },
        ],
        tools: [{
          type: 'function',
          function: {
            name: 'calculator',
            description: 'è®¡ç®—æ•°å­¦è¡¨è¾¾å¼',
            parameters: {
              type: 'object',
              properties: { expression: { type: 'string' } },
              required: ['expression'],
            },
          },
        }],
        tool_choice: 'auto',
        max_tokens: 200,
      });

      console.log('ğŸ“ å·¥å…·è°ƒç”¨:', JSON.stringify(response.choices[0].message, null, 2));
      expect(response.choices[0].message).toBeDefined();
    }, 30000);

    it('åº”è¯¥èƒ½è°ƒç”¨å¤©æ°”å·¥å…·', async (ctx) => {
      if (skipIfNoNetwork(ctx)) return;
      
      const response = await provider!.chat({
        model: OLLAMA_CONFIG.models[0],
        messages: [
          { role: 'system', content: 'ä½¿ç”¨ get_weather å·¥å…·æŸ¥å¤©æ°”ã€‚' },
          { role: 'user', content: 'ä¸Šæµ·å¤©æ°”ï¼Ÿ' },
        ],
        tools: [{
          type: 'function',
          function: {
            name: 'get_weather',
            description: 'æŸ¥è¯¢å¤©æ°”',
            parameters: {
              type: 'object',
              properties: { city: { type: 'string' } },
              required: ['city'],
            },
          },
        }],
        tool_choice: 'auto',
        max_tokens: 200,
      });

      console.log('ğŸ“ å¤©æ°”å·¥å…·æµ‹è¯•');
      expect(response.choices[0].message).toBeDefined();
    }, 30000);
  });

  describe('æµå¼è¾“å‡º', () => {
    it('åº”è¯¥èƒ½è¿›è¡Œæµå¼å¯¹è¯', async (ctx) => {
      if (skipIfNoNetwork(ctx)) return;
      
      const stream = await provider!.chatStream({
        model: OLLAMA_CONFIG.models[0],
        messages: [{ role: 'user', content: 'å†™4è¡Œè¯—' }],
        max_tokens: 100,
        stream: true,
      });

      let content = '';
      let chunks = 0;

      console.log('ğŸ“ æµå¼è¾“å‡º:');
      for await (const chunk of stream) {
        const c = chunk.choices?.[0]?.delta?.content;
        if (c) {
          content += c;
          chunks++;
          process.stdout.write(c);
        }
      }
      console.log(`\n  (${chunks} chunks)`);

      expect(content.length).toBeGreaterThan(0);
    }, 60000);
  });

  describe('é”™è¯¯å¤„ç†', () => {
    it('åº”è¯¥å¤„ç†æ— æ•ˆæ¨¡å‹', async (ctx) => {
      if (skipIfNoNetwork(ctx)) return;
      
      await expect(
        provider!.chat({
          model: 'invalid-model-xyz',
          messages: [{ role: 'user', content: 'test' }],
        })
      ).rejects.toThrow();
    }, 60000); // Ollama æ‹‰å–æ¨¡å‹å¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´æ‰ä¼šå¤±è´¥
  });

  describe('å¥åº·æ£€æŸ¥', () => {
    it('åº”è¯¥è¿”å›å¥åº·çŠ¶æ€', async (ctx) => {
      if (skipIfNoNetwork(ctx)) return;
      
      const healthy = await provider!.healthCheck();
      console.log('ğŸ“ å¥åº·:', healthy ? 'âœ…' : 'âŒ');
      expect(typeof healthy).toBe('boolean');
    }, 10000);
  });
});

describe('æ€§èƒ½æµ‹è¯•', () => {
  it('åº”è¯¥å¿«é€Ÿå“åº”', async (ctx) => {
    if (skipIfNoNetwork(ctx)) return;
    
    const start = Date.now();
    await provider!.chat({
      model: OLLAMA_CONFIG.models[0],
      messages: [{ role: 'user', content: '1+1' }],
      max_tokens: 10,
    });
    const ms = Date.now() - start;
    console.log(`ğŸ“ å“åº”: ${ms}ms`);
    expect(ms).toBeLessThan(30000);
  }, 35000);
});
